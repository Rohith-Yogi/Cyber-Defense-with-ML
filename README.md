# Cyber-Defense-with-ML
Attack and Defense models - Final Project/Challenge

Defenders Challenge Specifications:

Deliverable: Self-contained docker image with model querying via HTTP requests.
Goals:
     FPR: 1%
     TPR: 95%

Constraints:
     Memory: 1 GB max RAM
     Response time: 5 seconds per sample
          Warning: Timeouts will be considered evasions.


Attackers Challenge Specifications:

Deliverable: Evasive Malware Binaries.
Goals:
     Evade the most models possible.

Constraints:
     Maximum file size: 5MB of appended data.
     Evasive sample execution in the sandbox must be equivalent to the original sample.

Minimum score for grading:
   At least one sample must bypass at least one model.

   

Final Project/Challenge grading

Each group should deliver:
o A trained ML model to detect malware samples (defense step): 40%
o Modified malware samples intended to bypass other studentâ€™s detectors (attack step): 40%
o A presentation explaining their strategies (attack and defense): 20%

Evaluation happens in a tournament. The grade is the greatest score achieved. For the defensepart:
o Non-functional models will not receive a positive score.
o Any functional model that achieves FPR<1% and TPR>95% for the non-evasive samples distributed by the professor will receive at least a minimum score: 50.
o Any model that detects at least one Adversarial Example (AE) will receive a 60 score.
o Any model that detects at least one AE from each adversary team will receive a 70 score.
o Any model that detects all AEs from an adversary team but was evaded by the others receives a 80 score.
o Any model that detects at least 90% of all AEs receives a 90 score.
o Any model that detects more than 95% of all AEs receives a 100 score.
o Extra Points: Additional Models.

Groups might submit up to 5 additional defensive models.
Each additional model might receive a bonus of up to 10% (proportional to the
detection performance)

Additional models will be evaluated using the same criteria stated above.
Additional models must be significantly different (in features) of the primary model.

For the attacking samples:
o If any sample evaded at least one adversarial model, the team receives a minimum score: 50.
o If the samples evaded all classifiers of a single model, the team receives a 60 score.
o If the team evaded all classifiers with at least one sample, the team receives a 70 score.
o If the samples evaded at least 50% of all classifiers, the team receives a 80 score.
o If the samples evaded at least 90% of all classifiers, the team receives a 90 score.
o If the samples evaded all classifiers, the team scores a 100.
o The malware samples should be functional with regards to the execution in a sandbox environment.

o Extra points:

Any functional, automatically-generated bypass of a model counts as a full bypass for that given model.
Groups that demonstrate bypasses to their own models might earn up to 30% extra points (propotional to the number of bypasses)
